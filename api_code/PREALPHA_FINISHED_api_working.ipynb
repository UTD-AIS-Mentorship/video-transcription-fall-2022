{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbk9VuDIK7te"
      },
      "source": [
        "# FIRST: Go to Runtime -> Change Runtime type -> GPU accelerated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycp2tGL7LIra"
      },
      "source": [
        "This next codeblock installs an archived version of transformers\n",
        "\n",
        "installs datasets==1.16.1 for putting example datasets (might not be needed)\n",
        "\n",
        "bitsandbytes for tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDyXxk4hJ3zx",
        "outputId": "405024d3-52b5-49f8-ef68-fdb823aa1c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/deniskamazur/transformers/archive/gpt-j-8bit.zip\n",
            "  Downloading https://github.com/deniskamazur/transformers/archive/gpt-j-8bit.zip\n",
            "\u001b[K     \\ 10.2 MB 647 kB/s\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (4.13.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 74.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.17.0.dev0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.17.0.dev0) (3.9.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.17.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.17.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.17.0.dev0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.17.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.17.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.17.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.17.0.dev0) (1.2.0)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.17.0.dev0-py3-none-any.whl size=3653108 sha256=eb0d2406af3c628d776260faf851d454da593663c3acf096471d1eea00c61d11\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-oz0imjx_/wheels/f7/18/de/b62fe0bb6c170d5f315f8673652d2bfc776b847b3fd26553fb\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=f13395713509b0de2f4785c244d4aba9d24b3df6f488a09e68ea4ce894bcb518\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-oz0imjx_/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 sacremoses-0.0.53 tokenizers-0.13.1 transformers-4.17.0.dev0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets==1.16.1\n",
            "  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (4.13.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (1.21.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (3.8.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (0.10.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (1.3.5)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 84.4 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (0.3.5.1)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (6.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (21.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.16.1) (2022.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (4.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (1.8.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.16.1) (22.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.16.1) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.16.1) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.16.1) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.16.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.16.1) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.16.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.16.1) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.16.1) (3.9.0)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.16.1) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.16.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.16.1) (1.15.0)\n",
            "Installing collected packages: dill, xxhash, multiprocess, datasets\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.5.1\n",
            "    Uninstalling dill-0.3.5.1:\n",
            "      Successfully uninstalled dill-0.3.5.1\n",
            "Successfully installed datasets-1.16.1 dill-0.3.6 multiprocess-0.70.14 xxhash-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.35.0-py3-none-any.whl (62.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 62.5 MB 1.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.35.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y transformers && pip install --no-cache-dir https://github.com/deniskamazur/transformers/archive/gpt-j-8bit.zip\n",
        "!pip install datasets==1.16.1 # for loading example datasets (might not be needed)\n",
        "!pip install bitsandbytes # for tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgRgqz3VMCja"
      },
      "source": [
        "# Below loads model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "86U7y3C8KEWB"
      },
      "outputs": [],
      "source": [
        "def gpu_model():\n",
        "  import torch\n",
        "  import transformers\n",
        "  from transformers.models.gptj import GPTJForCausalLM #gptj only?\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu' # select gpu otherwise use cpu\n",
        "  tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\") #loads original 6B model\n",
        "  model = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\", low_cpu_mem_usage=True).to(device) #use pretrained 8bit \"shrunken\" model used for low memory aka using for google colab\n",
        "  return [device,tokenizer,model]\n",
        "  \n",
        "def cpu_model():\n",
        "  import torch\n",
        "  import transformers\n",
        "  from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu' # select gpu otherwise use cpu\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "  return [device,tokenizer,model]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAX5epM_MOMR"
      },
      "source": [
        "# Install youtubetranscript api for text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHbKbC1sL_Xq",
        "outputId": "768b4477-6090-43ab-b848-1f6f2a086782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-0.4.4-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from youtube_transcript_api) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (2.10)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.4.4\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube_transcript_api"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cIuztjAPTme"
      },
      "source": [
        "# Summary pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4SWK6SrNJhv",
        "outputId": "0b28405f-3008-43d4-b12f-b56fe5db1254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.85.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting nest-asyncio\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[K     |████████████████████████████████| 745 kB 19.2 MB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.19.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting starlette==0.20.4\n",
            "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from fastapi) (1.9.2)\n",
            "Collecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.20.4->fastapi) (4.1.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi) (2.10)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (6.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn) (7.1.2)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=5126dda98fa0ce6754360717ba8ce488b38e5be406e6ed8fb1ba0bad083da723\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: sniffio, anyio, starlette, h11, uvicorn, pyngrok, nest-asyncio, fastapi\n",
            "Successfully installed anyio-3.6.2 fastapi-0.85.1 h11-0.14.0 nest-asyncio-1.5.6 pyngrok-5.1.0 sniffio-1.3.0 starlette-0.20.4 uvicorn-0.19.0\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn\n",
        "!ngrok authtoken 2B39YnQIGeHG0FeFnLIgkPVJmdq_3eXfqZe6aXCu8YkFZ2nQ9\n",
        "\n",
        "# !pip install flask\n",
        "# !pip install flask-ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyB2T7RRUOoB",
        "outputId": "c95fdd5d-80b9-4a34-bfef-e39b6e98727f",
        "scrolled": true,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1019, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1653, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1611, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/ngrok\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyngrok/ngrok.py\", line 501, in main\n",
            "    run(sys.argv[1:])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyngrok/ngrok.py\", line 489, in run\n",
            "    process.run_process(pyngrok_config.ngrok_path, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyngrok/process.py\", line 339, in run_process\n",
            "    subprocess.call(start)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 341, in call\n",
            "    return p.wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1032, in wait\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1647, in _wait\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!ngrok http -host-header=rewrite localhost:8000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-GOtNFWHfNz",
        "outputId": "ffaa356c-cdfe-4d20-a1c1-566145867d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWNd0jofKttT",
        "outputId": "d2d04e98-b3c2-4a59-ba01-372da2d155a2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: http://845e-35-247-3-235.ngrok.io\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [136]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2607:fb90:8ab6:c752:ccf8:897a:1df8:8470:0 - \"OPTIONS /items HTTP/1.1\" 200 OK\n",
            "<class 'dict'>\n",
            "INFO:     2607:fb90:8ab6:c752:ccf8:897a:1df8:8470:0 - \"POST /items HTTP/1.1\" 200 OK\n",
            "<class 'dict'>\n",
            "INFO:     2607:fb90:8ab6:c752:ccf8:897a:1df8:8470:0 - \"POST /items HTTP/1.1\" 200 OK\n",
            "<class 'dict'>\n",
            "INFO:     172.58.180.237:0 - \"POST /items HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [136]\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, Request\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import torch\n",
        "from typing import Union\n",
        "from pydantic import BaseModel\n",
        "import re\n",
        "from transformers import pipeline\n",
        "from typing import TypeVar\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import JSONResponse\n",
        "import json\n",
        "\n",
        "\n",
        "# load models and store in variables\n",
        "dev_tok_model = cpu_model()\n",
        "tokenizer = dev_tok_model[1]\n",
        "model = dev_tok_model[2]\n",
        "# \n",
        "\n",
        "T = TypeVar(\"T\")\n",
        "\n",
        "\n",
        "class Result:\n",
        "    def __init__(self, ok: bool, value: T = None, error: str = \"\"):\n",
        "        self.ok = ok\n",
        "        # If the result is ok, set the value. Otherwise, set the error\n",
        "        self.value = value if ok else None\n",
        "        self.error = error if not ok else \"\"\n",
        "\n",
        "    def __str__(self):\n",
        "        # If the result is ok, return the value. Otherwise, return the error\n",
        "        return f\"Ok: {self.value}\" if self.ok else f\"Err: {self.error}\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "    def is_ok(self):\n",
        "        return self.ok\n",
        "\n",
        "    def is_err(self):\n",
        "        return not self.ok\n",
        "\n",
        "    def unwrap(self):\n",
        "        if self.is_ok():\n",
        "            return self.value\n",
        "        else:\n",
        "            raise Exception(self.error)\n",
        "\n",
        "    def match(self, ok_func, err_func):\n",
        "        if self.is_ok():\n",
        "            return ok_func(self.value)\n",
        "        else:\n",
        "            return err_func(self.error)\n",
        "\n",
        "    # func must return a Result\n",
        "    def flat_map(self, func):\n",
        "        if self.is_ok():\n",
        "            return func(self.value)\n",
        "        else:\n",
        "            return self\n",
        "\n",
        "\n",
        "is_youtube_regex = re.compile(r\"https?://(www\\.)?youtube\\.com\")\n",
        "\n",
        "\n",
        "def is_youtube(link: str) -> bool:\n",
        "    \"\"\"Check if link is a YouTube link\"\"\"\n",
        "    return is_youtube_regex.match(link) is not None\n",
        "\n",
        "\n",
        "youtube_video_id_regex = re.compile(r\"https?://(?:(?:(?:www\\.youtube\\.com|m\\.youtube\\.com)/watch\\?v=([0-9A-Za-z_-]{10}[048AEIMQUYcgkosw]).*)|(?:youtu\\.be/([0-9A-Za-z_-]{10}[048AEIMQUYcgkosw])))\")\n",
        "\n",
        "\n",
        "def get_youtube_video_id(link: str) -> Result:\n",
        "    \"\"\"Get the video id from a YouTube link\"\"\"\n",
        "    if not is_youtube(link):\n",
        "        return Result(False, error=\"Not a youtube link\")\n",
        "    match = youtube_video_id_regex.match(link)\n",
        "    if match is None:\n",
        "        return Result(False, error=\"Invalid youtube link\")\n",
        "    return Result(True, value=match.group(1))\n",
        "\n",
        "\n",
        "def get_transcript(video_id: str) -> Result:\n",
        "    try:\n",
        "        text = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        # We need to transform the transcript into a string\n",
        "        return Result(True, value=\" \".join([line[\"text\"] for line in text]))\n",
        "    except Exception as e:\n",
        "        return Result(False, error=str(e))\n",
        "\n",
        "\n",
        "def get_summary(text: str) -> Result:\n",
        "    # Gen must be a transformers pipeline\n",
        "    try:\n",
        "        prompt = text + \"\\n TLDR: \"\n",
        "        # WARNING: This is designed around gpt-neo-125M, which can be called in the following way\n",
        "        # Other models may require different parameters, and different ways of obtaining the summary\n",
        "        # Read through the documentation of the models for proper usage\n",
        "        # summary = gen(prompt, do_sample=True, temperature=0.9, max_new_tokens=200)[0][\"generated_text\"]\n",
        "\n",
        "        \"\"\"needs change\"\"\"\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids #.cuda() # .cuda() is for gpu, remove if not needed, gpt-j-6b needs gpu.\n",
        "\n",
        "        gen_tokens = model.generate(\n",
        "\n",
        "            input_ids,\n",
        "\n",
        "            do_sample=True,\n",
        "\n",
        "            temperature=0.9,\n",
        "\n",
        "            max_length=2000,\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "        summary = tokenizer.batch_decode(gen_tokens)[0]\n",
        "        \"\"\"needs change\"\"\"\n",
        "\n",
        "        return Result(True, value=summary)\n",
        "    except Exception as e:\n",
        "        return Result(False, error=str(e))\n",
        "\n",
        "\n",
        "def link_to_summary(link: str) -> Result:\n",
        "    return get_youtube_video_id(link) \\\n",
        "        .flat_map(get_transcript) \\\n",
        "        .flat_map(lambda text: get_summary(text))\n",
        "\n",
        "\n",
        "def link_to_transcript(link: str) -> Result:\n",
        "    return get_youtube_video_id(link) \\\n",
        "        .flat_map(get_transcript)\n",
        "\n",
        "\n",
        "def link_to_prompt(link: str) -> Result:\n",
        "    return get_youtube_video_id(link) \\\n",
        "        .flat_map(get_transcript) \\\n",
        "        .flat_map(lambda text: Result(True, value=text + \"\\n TLDR: \"))\n",
        "\n",
        "\n",
        "def unwrap_result(result: Result):\n",
        "    if result.is_ok():\n",
        "        return result.value\n",
        "    else:\n",
        "        raise Exception(result.error)\n",
        "\n",
        "\n",
        "def main(link):\n",
        "    #['audio-classification', 'automatic-speech-recognition', 'conversational', 'feature-extraction', 'fill-mask', 'image-classification', 'image-segmentation', 'ner', 'object-detection', 'question-answering', \n",
        "    #'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text2text-generation', 'token-classification', 'translation', 'zero-shot-classification', 'translation_XX_to_YY']\n",
        "    # pipe = pipeline('text-generation', model='EleutherAI/gpt-neo-125M') # need to remove this line?\n",
        "    result = link_to_summary(link)\n",
        "    return result\n",
        "\n",
        "# load server app\n",
        "app = FastAPI()\n",
        "# enable cors\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "@app.get('/index')\n",
        "async def home():\n",
        "  result = main('https://www.youtube.com/watch?v=P7yM0TKvUm4')\n",
        "  return result\n",
        "\"\"\"\n",
        "class Item(BaseModel):\n",
        "    id: Union[int, None] = None\n",
        "    link: str\n",
        "    summary: Union[str, None] = None\n",
        "    rating: Union[int, None] = None\n",
        "\n",
        "@app.post(\"/items\")\n",
        "async def getInformation(item : Request):\n",
        "    req_info = await item.json()\n",
        "    # persist id\n",
        "    # delete link\n",
        "    # retreive summary\n",
        "    # keep same rating.\n",
        "    print(type(req_info))\n",
        "    # link = req_info['link'] #put back\n",
        "    # summary = main(link) #put back\n",
        "    req_info['summary'] = \"summary\" #put back\n",
        "    req_info['link'] = \"\"\n",
        "    \n",
        "    return {\n",
        "        \"status\" : \"SUCCESS\",\n",
        "        \"data\" : req_info,\n",
        "        \"type\" : str(type(req_info))\n",
        "    }\n",
        " \"\"\"\n",
        "\n",
        "\n",
        "@app.post(\"/items\")\n",
        "async def getInformation(item : Request):\n",
        "    req_info = await item.json()\n",
        "    # persist id\n",
        "    # delete link\n",
        "    # retreive summary\n",
        "    # keep same rating.\n",
        "    print(type(req_info))\n",
        "    link = req_info['link']\n",
        "    summary = main(link)\n",
        "    req_info['summary'] = summary \n",
        "    req_info['link'] = \"\"\n",
        "    \n",
        "    return {\n",
        "        \"status\" : \"SUCCESS\",\n",
        "        \"data\" : req_info,\n",
        "        \"type\" : str(type(req_info))\n",
        "    }\n",
        "\n",
        "# @app.post(\"/items/\")\n",
        "# async def create_item(item: Item):\n",
        "#     return item\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VcM_hNwOO16c",
        "outputId": "5775ef22-1788-4190-8762-2ab8f12003bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the wealthiest circles \n",
            "of Victorian England,\n",
            "In the wealthiest circles \n",
            "of Victorian England,bizarre fads ran rampant.But perhaps none was as strange\n",
            "as the tapeworm diet,in which dieters swallowed\n",
            "an unhatched tapewormand let it grow inside them \n",
            "by consuming undigested meals.Obviously, this is an exceptionally\n",
            "dangerous and unhealthy wayto manage your weight.However, while modern fad diets\n",
            "aren't usually this extreme,they do promise similar results;\n",
            "specifically, losing weight fast.So, are there any fast diets that do work?And are any of them \n",
            "actually healthy for you?To answer these questions, \n",
            "let’s consider a thought experiment.Sam and Felix are identical twins\n",
            "both planning to go on a diet.They share the same height, weight,\n",
            "fat and muscle mass.But Sam is hoping to lose weight slowly,\n",
            "while Felix wants to go fast.Sam's plan is to gradually decrease\n",
            "his calorie intakeand increase his regular exercise.With less energy coming in\n",
            "and more being expended,he’s creating an energy deficit\n",
            "inside his body.To compensate, Sam’s body begins breaking \n",
            "down his emergency glucose supply,stored in the liver \n",
            "in the form of glycogen.Then, after 4 to 6 hours, \n",
            "his body starts burning fat cellsas a major energy source.This process releases lipid dropletswhich are broken down into compounds\n",
            "that float through the bloodstreamand provide energy to organs and tissues.Felix aims to create \n",
            "a similar energy deficitby dramatically cutting \n",
            "his calorie intake.Unlike Sam, \n",
            "who’s still eating smaller meals,Felix is eating almost nothing.And his body responds by going\n",
            "into a starvation response.Felix’s body breaks down his entire store\n",
            "of emergency glucose in just 18 hours.And while Sam steadily replenishes\n",
            "glycogen with every healthy meal,Felix’s low-calorie diet does not.Desperate for energy, his body starts\n",
            "breaking down other materials,including his muscles.Meanwhile, Sam’s regular exercise \n",
            "is maintaining his muscle mass.This means he’ll use more energy\n",
            "both during exercise and at rest,making it easier for him to lose weight.Felix, on the other hand,\n",
            "is losing muscle massand burning fewer calories than ever\n",
            "for his body's basic functions,making weight loss even more difficult.Despite all this, there’s one element\n",
            "of Felix’s fast dietthat might make him think\n",
            "he's on the right track.Every gram of glycogen is bound\n",
            "to several grams of water.This can add up to two kilograms\n",
            "of water weight,all of which is lost \n",
            "when the glycogen is depleted.For Felix, this might seem like\n",
            "he’s losing weight fast.But as soon as he stops starving himself,his body will replenish its glycogen store\n",
            "and regain that weight.Clearly, Felix’s plan does\n",
            "more harm than good,but extreme calorie reduction diets\n",
            "aren’t the only regimenspromising to shed weight fast.Plans called “detoxification diets” \n",
            "either promote or restrict certain foodsto provide specific nutrients\n",
            "in high quantities.These can be useful for addressing\n",
            "some nutritional problems,but they’re far too specific \n",
            "to be used as general cure-alls.For example, for a person \n",
            "with low vitamin A,a juice diet might be helpful.But for someone high in vitamin A,\n",
            "juicing could be disastrous.And regardless of personal nutrition,maintaining a juice diet \n",
            "over multiple weeksis likely to compromise the immune systemdue to a lack of essential fats\n",
            "and proteins.Therein lies the problem \n",
            "with all these fast-moving diets—whether you’re cutting calories\n",
            "or food groups,extreme diets are a shock to your system.There are well-established rates \n",
            "of healthy weight lossmotivated by both diet and exercisethat account for genetic and medical\n",
            "differences.And staying on those timelines requires \n",
            "a dietary lifestyle that’s sustainable.In fact, some of the worst side effects\n",
            "of extreme dietsare rarely discussed since so few people\n",
            "stick with them,it also bears mentioning that many\n",
            "societies have unhealthy relationshipswith weight,and people are often pressured to diet for\n",
            "reasons other than health or happiness.So rather than trying to lose weight fast,we should all be taking our time \n",
            "to figure outwhat the healthiest lifestyle\n",
            "is for ourselves.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-8-1bab6eb46b9d>\", line 31, in <module>\n",
            "    max_length=2000,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\", line 1103, in generate\n",
            "    inputs_tensor, model_kwargs, model_input_name\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\", line 510, in _prepare_encoder_decoder_kwargs_for_generation\n",
            "    model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\", line 828, in forward\n",
            "    output_attentions=output_attentions,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\", line 320, in forward\n",
            "    hidden_states = self.fc2(hidden_states)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 742, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 395, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
            "    if not islink(newpath):\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 171, in islink\n",
            "    st = os.lstat(path)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ],
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "raw_data = YouTubeTranscriptApi.get_transcript(\"P7yM0TKvUm4\")\n",
        "\n",
        "# print(raw_data)\n",
        "type(raw_data)\n",
        "# first part of transcript data\n",
        "print(raw_data[0][\"text\"])\n",
        "text = \"\"\n",
        "lis_text = []\n",
        "for i, e in enumerate(raw_data):\n",
        "  lis_text.append(raw_data[i][\"text\"])\n",
        "# list to string\n",
        "text = ''.join(lis_text)\n",
        "print(text)\n",
        "\n",
        "\n",
        "\n",
        "prompt = text + \"\\n TLDR:\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids #.cuda() # .cuda() is for gpu, remove if not needed, gpt-j-6b needs gpu.\n",
        "\n",
        "gen_tokens = model.generate(\n",
        "\n",
        "    input_ids,\n",
        "\n",
        "    do_sample=True,\n",
        "\n",
        "    temperature=0.9,\n",
        "\n",
        "    max_length=2000,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXPW319sPyyb"
      },
      "source": [
        "FROM THE ABOVE: we print the type and len of input_ids because unsure what it does\n",
        "\n",
        "FROM THE ABOVE: .cuda() function uses our gpu, (if you delete it, code might still work?)\n",
        "\n",
        "BELOW: print gen_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzsM9tNNPrmr"
      },
      "outputs": [],
      "source": [
        "print(type(input_ids))\n",
        "print(len(input_ids))\n",
        "print(gen_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMpGe5NHHfN5"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "tensorflow.test.is_gpu_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emb4Mqc7Px9-"
      },
      "source": [
        "# Things that need doing...?\n",
        "\n",
        "For App:\n",
        "- Ability to make API call to this model\n",
        "- Ability to append Summaries to database\n",
        "\n",
        "For Experiments:\n",
        "- Code that lets us loop through our summaries\n",
        "- Metrics for different models? Many metrics for same model? Different prompts?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "default:Python",
      "language": "python",
      "name": "conda-env-default-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}